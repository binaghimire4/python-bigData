{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grsIrrZ9RoU-"
      },
      "source": [
        "# Dask Machine Learning\n",
        "\n",
        "- Dask-ML enables scalable machine learning\n",
        "- It comes with explicit support for certain models such as dask-xgboost\n",
        "- It supports existing machine learning methods such as scikit-learn, tensorflow, keras, etc. \n",
        "- Large Model (Exploit parallelism with delayed executions, Hyperparameter tunning, etc.,)\n",
        "- Large Data (Dask Collections to manage memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7bACPCvRoVF"
      },
      "source": [
        "#### CPU bound vs MEM bound Machine Learning Models\n",
        "<img src=\"https://raw.githubusercontent.com/dmbala/python-bigData/main/Figures/cpu_mem_bound.png\" width=500 height=400>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-smXIoMRoVF"
      },
      "source": [
        "#### Distributed Machine Learning across multiple nodes\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/dmbala/python-bigData/main/Figures/DaskDistributedJob.png\" width=500 height=200>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3w764SSRoVG"
      },
      "source": [
        "### Compute Bound\n",
        "\n",
        "- Distribute training and prediction across multiple nodes. \n",
        "- Hyperparameter tunning\n",
        "\n",
        "### Memory Bound \n",
        "- Blockwise Ensemble Methods\n",
        "- Incremental Learning\n",
        "\n",
        "### Compute and Memory Bound\n",
        "Re-implemented Models like dask-xgboost and dask-knn are efficeint with both CPU and Memor intensive computations. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dask-ml"
      ],
      "metadata": {
        "id": "8Ljs_bUvwTnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install memory_profiler"
      ],
      "metadata": {
        "id": "pa6Hmwouwc71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgUFqD5JRoVH"
      },
      "outputs": [],
      "source": [
        "# Importing dask \n",
        "import dask\n",
        "import dask.array as da\n",
        "import dask.dataframe as dd\n",
        "import dask.delayed as delayed\n",
        "import dask_ml.datasets\n",
        "import dask_ml.cluster\n",
        "import time\n",
        "%load_ext memory_profiler\n",
        "dask.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Erlm3NndRoVJ"
      },
      "source": [
        "## Blockwise Ensemble Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPuTE_39RoVK"
      },
      "source": [
        "- Ensemble methods such as Bagging methods, Forrests of randomized trees, etc., are good for blockwise approaches. \n",
        "- Create homogenous data blocks from dask.array or dask.dataframe. \n",
        "- Train a copy of the model on each block. \n",
        "- At prediction, take an ensemble average of the trainined models. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rT-X6xHiRoVL"
      },
      "outputs": [],
      "source": [
        "# A classification example from dask_ml.datasets\n",
        "X, y = dask_ml.datasets.make_classification(n_samples=1e4, chunks=1e3, random_state=0)\n",
        "X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9iwl6QhRoVM"
      },
      "source": [
        "The sub-estimator should be an instantiated scikit-learn-API compatible estimator (anything that implements the fit / predict API, including pipelines). It only needs to handle in-memory datasets. Weâ€™ll use sklearn.linear_model.RandomForestClassifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ousm1U1bRoVN"
      },
      "outputs": [],
      "source": [
        "import dask_ml.ensemble\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "subestimator = RandomForestClassifier(random_state=0)\n",
        "clf = dask_ml.ensemble.BlockwiseVotingClassifier(\n",
        "    subestimator,\n",
        "    classes=[0, 1]\n",
        ")\n",
        "clf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWliZc9kRoVO"
      },
      "source": [
        "We can train the esemble of models on data chunks. This will independently fit a clone of subestimator on each partition of X and y."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXyzR8ERRoVP"
      },
      "outputs": [],
      "source": [
        "clf.fit(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwCpVpjHRoVQ"
      },
      "outputs": [],
      "source": [
        "clf.estimators_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtGpiEsgRoVS"
      },
      "source": [
        "Different estimators were trained on separate batches of data. Each estimator has its own set of parameters. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmrFT_xdRoVS"
      },
      "outputs": [],
      "source": [
        "preds = clf.predict(X[:20])\n",
        "preds.compute()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jw0Dl4bjRoVT"
      },
      "source": [
        "The prediction calls subestimator.predict(chunk) for each subestimator (20 in our case). These subestimator predictions are averaged at the end. \n",
        "\n",
        "The blockwise algorithm was applied to the training and the prediction steps. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAMZNhIoRoVT"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "%memit clf.score(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-u4b3IxRoVV"
      },
      "source": [
        "### Predictions on large data sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awMAcUeoRoVV"
      },
      "outputs": [],
      "source": [
        "#da.concatenate([X, X, X, X])\n",
        "N = 10\n",
        "X_large = da.concatenate([ X for _ in range(N)])\n",
        "y_large = da.concatenate([ y for _ in range(N)])\n",
        "X_large"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5C9rXujPRoVY"
      },
      "outputs": [],
      "source": [
        "X_large.rechunk(10000, 10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dk5n6BfARoVa"
      },
      "outputs": [],
      "source": [
        "y_large.rechunk(10000, 10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ole2tTzjRoVc"
      },
      "outputs": [],
      "source": [
        "clf.score(X_large, y_large)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3_2KHavRoVd"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "%memit clf.score(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCozpi4yRoVe"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "%memit clf.score(X_large, y_large)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpOlY1m4RoVe"
      },
      "source": [
        "## Incremental learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJSIwF22RoVe"
      },
      "source": [
        "- Some estimators are suitable for incremental training. This is useful for on-line training and as well training of large data sets. \n",
        "\n",
        "- Scikit-Learn provides partial_fit function for incremental learning. The partial_fit function works with Stochastic Gradient Descent, K-means, and Passive-Aggresive, and Naive Bayes based ML methods. \n",
        "\n",
        "- dask_ml.wrappers.Incremental acts as a bridge between Dask and Scikit-Learn estimators supporting the partial_fit API. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwRJJ2DNRoVf"
      },
      "outputs": [],
      "source": [
        "from dask_ml.wrappers import Incremental\n",
        "from sklearn.linear_model import SGDClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBiC6Ja_RoVf"
      },
      "outputs": [],
      "source": [
        "X, y = dask_ml.datasets.make_classification(n_samples=10000, chunks=1000, random_state=0)\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCQ_jMN5RoVg"
      },
      "outputs": [],
      "source": [
        "estimator = SGDClassifier(random_state=10, max_iter=100)\n",
        "clf = Incremental(estimator)\n",
        "clf.fit(X, y, classes=[0, 1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bv_vzArXRoVg"
      },
      "source": [
        "As usual with Dask-ML, scoring is done in parallel (and distributed on a cluster if youâ€™re connected to one)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Y5RkrDGRoVg"
      },
      "outputs": [],
      "source": [
        "clf.score(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htH48JekRoVh"
      },
      "source": [
        "## Hyper parameter Search -  Support Vector Classifier (CPU Bound)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/dmbala/python-bigData/main/Figures/svc.png\" width=500 height=400>\n",
        "\n",
        " https://www.datacamp.com/tutorial/svm-classification-scikit-learn-python"
      ],
      "metadata": {
        "id": "H0X5-oSvUhNB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3RXgCoJRoVi"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import pandas as pd\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHVTsv8QRoVj"
      },
      "outputs": [],
      "source": [
        "from dask.distributed import Client, LocalCluster\n",
        "client = Client(n_workers=2, threads_per_worker=2, memory_limit='4GB')\n",
        "client "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "co0qlpgGRoVj"
      },
      "outputs": [],
      "source": [
        "X, y = make_classification(n_samples=1000, random_state=0)\n",
        "X[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SiMGBwWLRoVk"
      },
      "outputs": [],
      "source": [
        "param_grid = {\"C\": [0.001, 0.01, 0.1, 1.0, 2.0],\n",
        "              \"kernel\": ['rbf', 'poly', 'sigmoid'],\n",
        "              \"shrinking\": [True, False]}\n",
        "\n",
        "grid_search = GridSearchCV(SVC(gamma='auto', random_state=0, probability=True),\n",
        "                           param_grid=param_grid,\n",
        "                           n_jobs=-1,\n",
        "                           cv=3)\n",
        "                           "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnQicxuFRoVl"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "grid_search.fit(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFc2KVdXRoVm"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "with joblib.parallel_backend('dask'):\n",
        "    grid_search.fit(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0vV04w-RoVm"
      },
      "outputs": [],
      "source": [
        "grid_search.score(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "by8kVeIeRoVm"
      },
      "outputs": [],
      "source": [
        "client.shutdown()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sYF4K8PRoVn"
      },
      "source": [
        "## Summary\n",
        "- Deploy dask-ml and dask collections to manage large data for the machine learning\n",
        "- Hyperparameter training of models can be accomplished by distributing the jobs on multiple machines"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}